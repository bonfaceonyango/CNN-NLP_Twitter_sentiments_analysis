{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c5f2de-4f52-4e0c-8f0a-d9b19f0889df",
   "metadata": {},
   "source": [
    "# Dataset description\n",
    "Data for this project is obtained from kaggle. The link to the data is https://www.kaggle.com/code/youben/twitter-sentiment-analysis/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fd567d-5d5c-4972-814e-d1173f058ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 18:56:13.524355: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-14 18:56:13.524383: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/bonface/miniconda3/envs/tensorflow/lib/python3.10/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "#Import required library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import string\n",
    "import matplotlib as plt\n",
    "import pydot \n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import regularizers\n",
    "from collections import Counter\n",
    "from tensorflow.keras import losses\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import  pad_sequences\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02a7669-fa33-47e8-bbb7-bfb26b356cf8",
   "metadata": {},
   "source": [
    "# Library description\n",
    "## Shutil\n",
    "The shutil in Python is a module used for  several functions mostly to to deal with  file operations  and their collections.It used to copy or  removal files.It resembkes OS module but the only difference is that the OS module lacks functions dealing with file collections\n",
    "<b>\n",
    "## Tensorflow library\n",
    " It is a library used for machine learnng and deep learning\n",
    "## String module\n",
    " It contains string functions for processing python string\n",
    "## Pydot\n",
    "This is an open source graph visualization software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b08ab23a-f93f-4416-b84c-b9f76bc275db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5701ab59-a795-4aa4-b113-ed359b80c599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_103249/91792818.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 18:56:16.048911: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 18:56:16.048948: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-07-14 18:56:16.048982: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n"
     ]
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d765628a-af1d-44d6-bd26-a2213511ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove emoji preprocessing code\n",
    "import emoji\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "#text = give_emoji_free_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59e8c56-999d-49ed-9b08-6c9e414f8596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove url\n",
    "def rem_url(data_clean):\n",
    "    p=re.compile(r'\\<http.+?\\>', re.DOTALL)\n",
    "    return re.sub(p, '',data_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ba086e1-d6ec-48c8-9491-3be4ecda3c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuations\n",
    "from string import punctuation\n",
    "type(punctuation)\n",
    "my_punctuation = punctuation.replace(\"'\", \"\")\n",
    "my_punctuation\n",
    "def rem_punct(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", my_punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe083990-dcc6-4858-bdd5-05d0f32ed32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'encoding': 'utf-8', 'confidence': 0.99, 'language': ''}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load non_UTF8 data file\n",
    "#Check the character encoding type\n",
    "data_path=\"train.csv\"\n",
    "import chardet\n",
    "with open(data_path, 'rb') as rawdata:\n",
    "    result = chardet.detect(rawdata.read(100000))\n",
    "result\n",
    "#From output below, encoding is windows-1252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7493f373-3ef2-4cce-b2d0-f96fe56ccb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# data=pd.read_csv(\"train.csv\")\n",
    "data = pd.read_csv(\"train.csv\",encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3be19893-9ab6-46e2-a76c-69053b5f6634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "305664d4-75ea-47ba-a16c-9ca0f1658907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textID           0\n",
       "text             0\n",
       "selected_text    0\n",
       "sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check null values\n",
    "data.isnull().sum()\n",
    "#Drop nall\n",
    "data=data.dropna()\n",
    "# Check after dropping null\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "de84cd53-9b61-45a7-9b49-5fb9ccfa5a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27480, 4)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61542272-e3d4-40cb-a4a8-d9cbf77c0201",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=0,how=\"any\",inplace=True)\n",
    "# Remove wors less than 2\n",
    "data[\"word_count\"]=data[\"text\"].apply(lambda x:len(str(x)))\n",
    "mask=data[\"word_count\"]>2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "edbf3938-14a3-4cce-a9eb-60d49081bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[mask]\n",
    "data[\"text\"]=(data[\"text\"].\n",
    "      apply(rem_url).\n",
    "      apply(give_emoji_free_text).\n",
    "      apply(rem_punct)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "062125c0-6b7c-4210-8b4b-70af21f94323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "neutral      11117\n",
       "positive      8582\n",
       "negative      7781\n",
       "dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[[\"sentiment\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "34ccb2e4-3750-4d97-abef-42a1675d32e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27480, 5)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e719274-f745-405e-8451-4acc76b6b7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_sent=data[\"word_count\"].max()\n",
    "longest_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a00bf74-4f8d-4705-866a-c7704cf342ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove emoji\n",
    "# def delete_emoji(data):\n",
    "#     emoji= re.compile(\"[\"\n",
    "#             u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "#             u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "#             u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "#             u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "#             u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "#             u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "#             u\"\\U0001F600-\\U0001F64F\"\n",
    "#             u\"\\U00002702-\\U000027B0\"\n",
    "#             u\"\\U000024C2-\\U0001F251\"\n",
    "#             u\"\\U0001f926-\\U0001f937\"\n",
    "#             u\"\\U0001F1F2\"\n",
    "#             u\"\\U0001F1F4\"\n",
    "#             u\"\\U0001F620\"\n",
    "#             u\"\\u200d\"\n",
    "#             u\"\\u2640-\\u2642\"\n",
    "#             \"]+\", flags=re.UNICODE)\n",
    "\n",
    "#     return emoji.sub(r'',data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ebfe919-0af0-454b-b26d-972dc17b7397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3534, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test= pd.read_csv(\"test.csv\",encoding=\"utf-8\")\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "79b90ada-1e0d-42af-ae1a-1fdd75283c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing test data\n",
    "# Check null values\n",
    "test.isnull().sum()\n",
    "#Drop nall\n",
    "test=data.dropna()\n",
    "# Check after dropping null\n",
    "test.isnull().sum()\n",
    "test.dropna(axis=0,how=\"any\",inplace=True)\n",
    "# Remove wors less than 2\n",
    "test[\"word_count\"]=test[\"text\"].apply(lambda x:len(str(x)))\n",
    "mask=test[\"word_count\"]>2\n",
    "\n",
    "\n",
    "test=test[mask]\n",
    "test[\"text\"]=(test[\"text\"].\n",
    "      apply(rem_url).\n",
    "      apply(give_emoji_free_text).\n",
    "      apply(rem_punct)\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18f5e4e6-7702-4b00-9226-6797c7b7e41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     11112\n",
       "positive     8582\n",
       "negative     7780\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2e9ba832-0ff7-4bd7-b089-e62c429b006e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longest_sent_test=test[\"word_count\"].max()\n",
    "longest_sent_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b7f6cdd-c41e-4134-8b30-684902f3d2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Tokenizing.\n",
    "# Splitting sentense in words \n",
    "words=3000\n",
    "token=Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',num_words=words,lower=True,oov_token=None)\n",
    "token.fit_on_texts(data[\"text\"].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cbb619f4-72c8-450a-b3ba-ae465bfbca23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 469, 16, 68, 1, 119, 45]]\n"
     ]
    }
   ],
   "source": [
    "print(token.texts_to_sequences([\"I`d have responded, if I were going\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4444fcc7-b5a0-4364-89d8-58288962af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data[\"text\"].tolist()\n",
    "y=data[\"sentiment\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019aa83-0c33-495e-84da-f925b21716ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ac548614-4eb3-4528-861a-6eb79d450be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output is the seuence of indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3cbb4728-ce00-4bb9-95a5-3f914f0e5a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42,stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e53421c-d494-4009-a769-162fa1f98547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding add 0 to sebtenses with less words in the set\n",
    "X_train=np.array(token.texts_to_sequences(X_train),dtype=\"object\")\n",
    "X_train=np.array(token.texts_to_sequences(X_test),dtype=\"object\")\n",
    "test=np.array(token.texts_to_sequences(test[\"text\"].tolist()),dtype=\"object\")\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7f3ed-4fff-490d-b820-88b4c67fe68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4e9690f9-9c91-4954-8509-369594f7610f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 559, 5, 237, 1872, 47, 14, 758, 3, 24, 351, 16, 170, 1702, 2, 8]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3cbcd5b-2370-4435-96c0-1069847a278a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'IN PAIN my big toe got stomped on during the hokey cokeu its throbbing anyone have any suggestions to heal it'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Padding\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#X_train=pad_sequences(X_train,padding=\"post\",maxlen=145)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m X_test\u001b[38;5;241m=\u001b[39m\u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m145\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/keras/utils/data_utils.py:1041\u001b[0m, in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1038\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTruncating type \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtruncating\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not understood\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;66;03m# check `trunc` has expected shape\u001b[39;00m\n\u001b[0;32m-> 1041\u001b[0m trunc \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m!=\u001b[39m sample_shape:\n\u001b[1;32m   1043\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of sample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrunc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of sequence at \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1044\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is different from expected shape \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1045\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'IN PAIN my big toe got stomped on during the hokey cokeu its throbbing anyone have any suggestions to heal it'"
     ]
    }
   ],
   "source": [
    "#Padding\n",
    "#X_train=pad_sequences(X_train,padding=\"post\",maxlen=145)\n",
    "X_test=pad_sequences(X_test,padding=\"post\",maxlen=145)\n",
    "#test=pad_sequences(test,padding=\"post\",maxlen=145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee18023f-34a4-4379-a0a6-aba5b67b2ae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 559, 5, 237, 1872, 47, 14, 758, 3, 24, 351, 16, 170, 1702, 2, 8]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c7f72fb-bd98-40f5-afeb-050c99b6bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding\n",
    "le=LabelEncoder()\n",
    "y_label=le.fit_transform(y)\n",
    "print(y_label[0:5])\n",
    "y_label=np.array(tf.keras.utils.to_categorical(y_label))\n",
    "y_label[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d077713-a408-445d-8ee1-09ecb72bff29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d3de3e-66ca-4630-8c9a-17f8c3a7b112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2398d-bc6a-4926-a694-f46c03b81311",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
